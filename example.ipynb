{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fee5b7c3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#  Benchmarking class-out-of-distribution performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e990c8f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this example we will see how to use our framework to benchmark C-OOD performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43747d7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To use the dataset from our paper (a subset of ImageNet-21k), first download ImageNet-21k in its entirety and use the *get_paper_dataset_info(path_to_ImageNet21k)* to trasnform it into our filtered version of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c905d6c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83012f0c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from cood_uncertainty_lib import benchmark_model_on_cood_with_severities\n",
    "import plotly.express"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using our framework to benchmark a model is as easy as using four lines of code:\n",
    "```\n",
    "    paper_dataset_ood = get_paper_ood_dataset_info(path_to_full_imagenet21k, skip_scan=True)\n",
    "    paper_dataset_id = get_paper_id_dataset_info(path_to_full_imagenet1k, skip_scan=False)\n",
    "    \n",
    "    results = benchmark_model_on_cood_with_severities(model='resnet50', cood_dataset_info=paper_dataset_ood,\n",
    "                                                    id_dataset_info=paper_dataset_id))\n",
    "    plotly.express.line(results, x='severity_levels', y='cood-auroc', color='model_name-kappa')\n",
    "```\n",
    "\n",
    "The above code will benchmark a pretrained ResNet-50 available in the timm repo ( https://github.com/rwightman/pytorch-image-models ) on ImageNet (the code is able to benchmark custom models as well, see below) and will plot its \"degradation graph\" (identical to Figure 1 from the paper).\n",
    "\n",
    "To save compute, in this example we will benchmark models on a smaller dummy dataset.\n",
    "To do that, we first need to define our custom dataset that consists of an ID part (in the complete dataset this part is made of classes from ImageNet-1k) and an OOD part (in the complete dataset this part is a filtered version of ImageNet-21k without the classes from ImageNet-1k):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from download_dummy_dataset import download_dummy_dataset\n",
    "download_dummy_dataset('./dummy_dataset')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# <path to images dir>/classname/*.(jpg|png|jpeg)\n",
    "dummy_ood_dataset_info = {\n",
    "    'dataset_name': 'Dummy_OOD',\n",
    "    'images_base_folder': './dummy_dataset/dummy_ood',  \n",
    "    'test_estimation_split_percentage': 0.25\n",
    "}\n",
    "\n",
    "dummy_id_dataset_info = {\n",
    "    'dataset_name': 'Dummy_ID',\n",
    "    'images_base_folder': './dummy_dataset/dummy_id',\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The 'dataset_name' value is used to name the metadata of the dataset. The metadata is helpful in saving compute for using the dataset.\n",
    "\n",
    "'test_estimation_split_percentage' defines the split between the amount of the OOD data used to estimate each class' severity (hardness) score and the amount used for testing its performance. For ImageNet, we used 25% for testing (50 samples) vs 75% for estimation (150 samples).\n",
    "\n",
    "Note that you can define your own custom datasets in a similar fashion."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's use our custom dummy dataset and evaluate an ImageNet pretrained ResNet-18 model with softmax as its confidence function:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = benchmark_model_on_cood_with_severities(model='resnet18',\n",
    "                                                    confidence_function='softmax',\n",
    "                                                    cood_dataset_info=dummy_ood_dataset_info,\n",
    "                                                    id_dataset_info=dummy_id_dataset_info)\n",
    "fig = plotly.express.line(results, x='severity level', y='ood-auroc', color='model name - confidence function')\n",
    "\n",
    "fig.update_layout(width=1000, height=400)\n",
    "fig.show('png')\n",
    "# Unfortunately, github doesn't load plotly plots on notebooks. This is why we show them as png files."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above plot is a 'degradation graph', showing the model's performance in AUROC (y axis) across different severity levels (x axis).\n",
    "\n",
    "Note that after running this code snippet, a new results csv was added at .\\models_results\\resnet18\\resnet18_softmax_n11.csv \n",
    "\n",
    "- The code first checks whether or not results for this specific combination of *(model, confidence function)* already exist."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also evaluate and compare different models at the same time.\n",
    "For example, suppose we want to compare two models, ResNet-18 and ResNet-34, each equipped with either softmax or entropy as its confidence function.\n",
    "\n",
    "In this case, we can pass lists containing the models and lists containing the confidence functions (instead of just passing one string), as follows:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = benchmark_model_on_cood_with_severities(model=['resnet18', 'resnet34'],\n",
    "                                                    confidence_function=['softmax', 'entropy'],\n",
    "                                                    cood_dataset_info=dummy_ood_dataset_info,\n",
    "                                                    id_dataset_info=dummy_id_dataset_info)\n",
    "fig = plotly.express.line(results, x='severity level', y='ood-auroc', color='model name - confidence function')\n",
    "\n",
    "fig.update_layout(width=1000, height=400)\n",
    "fig.show('png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that since we've already evaluated ResNet-18 with a softmax confidence function before, the same results were used for this current comparison rather than being calculated again."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating custom models and confidence functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Evaluating custom models:**\n",
    "There are a number of ways to pass the model to the *benchmark_model_on_cood_with_severities* method.\n",
    "- If it is a 'timm' model, its matching string name could be passed (as seen in the examples above).\n",
    "- Otherwise, the method expects a dictionary like the one in the example below:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
    "\n",
    "weights = MobileNet_V3_Small_Weights.DEFAULT\n",
    "transforms = weights.transforms()\n",
    "\n",
    "model = mobilenet_v3_small(weights=weights)\n",
    "\n",
    "example_model_input = {'model_name': 'mobilenet_v3_small', 'model': model, 'transforms': transforms}\n",
    "results = benchmark_model_on_cood_with_severities(model=example_model_input,\n",
    "                                                    confidence_function='softmax',\n",
    "                                                    cood_dataset_info=dummy_ood_dataset_info,\n",
    "                                                    id_dataset_info=dummy_id_dataset_info)\n",
    "fig = plotly.express.line(results, x='severity level', y='ood-auroc', color='model name - confidence function')\n",
    "\n",
    "fig.update_layout(width=1000, height=400)\n",
    "fig.show('png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The *'model_name'* argument defines the name for the directory into which the results will be saved in ./models_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Evaluating custom confidence functions:**\n",
    "We've implemented the following confidence functions in utils/confidence_functions.py:\n",
    "- Softmax response\n",
    "- Entropy\n",
    "- Max logit\n",
    "- ODIN [1]\n",
    "- MC Dropout [2]\n",
    "\n",
    "Additional confidence functions could be defined and passed as an argument for *'benchmark_model_on_cood_with_severities'*. \n",
    "The easiest way to define custom confidence functions is using *extract_custom_confidence_function_on_dataset*, defined in utils/confidence_functions, and pass to it your own custom confidence function.\n",
    "\n",
    "Your custom function should get a model and images as input, and return the model's logits and confidence scores as an output.\n",
    "\n",
    "Finally, similarly to when we passed custom models, we define a dictionary containing all information about our confidence function and then pass it to *benchmark_model_on_cood_with_severities* as an argument.\n",
    "\n",
    "\n",
    "In the following code snippets we show how to define our own custom softmax response as a confidence function:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "def custom_softmax_confidence_function(model, x):\n",
    "    logits = model(x)\n",
    "    probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "    softmax_conf, _ = torch.max(probs, dim=1)\n",
    "    return logits, softmax_conf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.confidence_functions import extract_custom_confidence_function_on_dataset\n",
    "import functools\n",
    "\n",
    "custom_softmax = functools.partial(extract_custom_confidence_function_on_dataset, \n",
    "                                   confidence_function=custom_softmax_confidence_function)\n",
    "custom_confidence_function = {'confidence_metric_name': 'custom_softmax', \n",
    "                              'confidence_metric_callable': custom_softmax}\n",
    "results = benchmark_model_on_cood_with_severities(model='resnet18',\n",
    "                                                    confidence_function=custom_confidence_function,\n",
    "                                                    cood_dataset_info=dummy_ood_dataset_info,\n",
    "                                                    id_dataset_info=dummy_id_dataset_info)\n",
    "fig = plotly.express.line(results, x='severity level', y='ood-auroc', color='model name - confidence function')\n",
    "\n",
    "fig.update_layout(width=1000, height=400)\n",
    "fig.show('png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can define multiple custom confidence functions and compare them.\n",
    "In the example below we define an inverse-softmax function as our custom confidence function, and compare its performance to softmax:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def custom_inverse_softmax_confidence_function(model, x):\n",
    "    logits = model(x)\n",
    "    probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "    softmax_conf, _ = torch.max(probs, dim=1)\n",
    "    return logits, -softmax_conf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "custom_inverse_softmax = functools.partial(extract_custom_confidence_function_on_dataset, \n",
    "                                   confidence_function=custom_inverse_softmax_confidence_function)\n",
    "custom_confidence_function2 = {'confidence_metric_name': 'custom_inverse_softmax', \n",
    "                              'confidence_metric_callable': custom_inverse_softmax}\n",
    "results = benchmark_model_on_cood_with_severities(model='resnet18',\n",
    "                                                    confidence_function=[custom_confidence_function, \n",
    "                                                                         custom_confidence_function2],\n",
    "                                                    cood_dataset_info=dummy_ood_dataset_info,\n",
    "                                                    id_dataset_info=dummy_id_dataset_info)\n",
    "fig = plotly.express.line(results, x='severity level', y='ood-auroc', color='model name - confidence function')\n",
    "\n",
    "fig.update_layout(width=1000, height=400)\n",
    "fig.show('png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The examples above were trivial (softmax and inverse softmax), but a more sophisticated confidence function (like ODIN) could also be implemented easily."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, if your confidence function requires more advanced features to work, you can define your own custom confidence function similarly to how we defined the existing functions in utils/confidence_functions.py.\n",
    "\n",
    "In the following code snippet we use *'extract_softmax_on_dataset'* as an example for a possible confidence function to be defined:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils.confidence_functions import extract_softmax_on_dataset\n",
    "custom_confidence_function = {'confidence_metric_name': 'softmax_response', \n",
    "                              'confidence_metric_callable': extract_softmax_on_dataset}\n",
    "results = benchmark_model_on_cood_with_severities(model='resnet18',\n",
    "                                                    confidence_function=custom_confidence_function,\n",
    "                                                    cood_dataset_info=dummy_ood_dataset_info,\n",
    "                                                    id_dataset_info=dummy_id_dataset_info)\n",
    "fig = plotly.express.line(results, x='severity level', y='ood-auroc', color='model name - confidence function')\n",
    "\n",
    "fig.update_layout(width=1000, height=400)\n",
    "fig.show('png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Advanced tricks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Evaluating specific severity levels:** If we want to make a similar comparison but we're only interested in specific severity levels, for example, 8-10, we can pass the desired levels as an argument to the method:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = benchmark_model_on_cood_with_severities(model=['resnet18', 'resnet34'],\n",
    "                                                    confidence_function=['softmax', 'entropy'],\n",
    "                                                    cood_dataset_info=dummy_ood_dataset_info,\n",
    "                                                    id_dataset_info=dummy_id_dataset_info,\n",
    "                                                    levels_to_benchmark=[8,9,10])\n",
    "fig = plotly.express.line(results, x='severity level', y='ood-auroc', color='model name - confidence function')\n",
    "\n",
    "fig.update_layout(width=1000, height=400)\n",
    "fig.show('png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Evaluating a custom number of severity levels:**\n",
    "In the paper we have used 11 severity levels for analysis, but we can define less \\ more levels to be used. This is done using the 'num_severity_levels' argument: "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = benchmark_model_on_cood_with_severities(model=['resnet18', 'resnet34'],\n",
    "                                                    confidence_function=['softmax', 'entropy'],\n",
    "                                                    cood_dataset_info=dummy_ood_dataset_info,\n",
    "                                                    id_dataset_info=dummy_id_dataset_info,\n",
    "                                                    num_severity_levels=21)\n",
    "fig = plotly.express.line(results, x='severity level', y='ood-auroc', color='model name - confidence function')\n",
    "\n",
    "fig.update_layout(width=1000, height=400)\n",
    "fig.show('png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that using a different number of severity levels changes the statistical interpretation of each severity level. In the above example the median severity is severity level 10 (while in the original 11 levels of severity, the median was severity level 5)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Loading paper results for 520 models on ImageNet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can easily load and compare results for the 520 ImageNet classifiers we have evaluated in the paper (using a filtered version of ImageNet-21k, see Section 4 in the paper). Here is an example of how to do this using the *get_paper_results* method:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from cood_uncertainty_lib import get_paper_results\n",
    "results = get_paper_results(model_name='resnet50', confidence_function='softmax')\n",
    "fig = plotly.express.line(results, x='severity level', y='ood-auroc', color='model name - confidence function')\n",
    "\n",
    "fig.update_layout(width=1000, height=400)\n",
    "fig.show('png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similarly, different models with different confidence functions could be evaluated as follows:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = get_paper_results(model_name=['resnet50', 'resnet18'], confidence_function=['softmax', 'odin'])\n",
    "fig = plotly.express.line(results, x='severity level', y='ood-auroc', color='model name - confidence function')\n",
    "\n",
    "fig.update_layout(width=1000, height=400)\n",
    "fig.show('png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, calling *get_paper_results()* with no arguments will return results for **all** combinations of *(model, confidence function)* evaluated in the paper."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using our benchmark with ImageNet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As mentioned at the beginning of this notebook, to use the dataset from our paper (a subset of ImageNet-21k), first download ImageNet-21k in its entirety and use the *get_paper_dataset_info(path_to_ImageNet21k)* to trasnform it into our filtered version of the data.\n",
    "Define the ID dataset to be ImageNet-1k entire validation set.\n",
    "Finally, use the same code as we've seen above to evaluate with these datasets.\n",
    "For example:\n",
    "\n",
    "```\n",
    "from cood_uncertainty_lib import benchmark_model_on_cood_with_severities, get_paper_ood_dataset_info, \\\n",
    "    get_paper_id_dataset_info\n",
    "\n",
    "path_to_imagenet1k = 'D:\\ImageNet_1K\\ILSVRC2012_img_val'  # path to where you've saved ILSVRC2012_img_val\n",
    "paper_id_dataset = get_paper_id_dataset_info(path_to_imagenet1k, skip_scan=False)\n",
    "\n",
    "path_to_imagenet21k = 'D:\\fall11_whole'  # path to where you've saved fall11_whole, AKA, ImageNet-21k\n",
    "paper_ood_dataset = get_paper_ood_dataset_info(path_to_imagenet21k, skip_scan=True)\n",
    "\n",
    "\n",
    "results = benchmark_model_on_cood_with_severities(model='resnet18',\n",
    "                                                    confidence_function='softmax',\n",
    "                                                    cood_dataset_info=paper_id_dataset,\n",
    "                                                    id_dataset_info=paper_ood_dataset)\n",
    "```\n",
    "\n",
    "* When loading the paper's dataset with *get_paper_ood_dataset_info* \\ *get_paper_id_dataset_info*, the method scans the dataset to validate its integrity by default. This might take some time, *get_paper_ood_dataset_info* especially. The argument *skip_scan* controls whether or not to make this scan. We recommend leaving it \"*False*\" the first time loading these datasets, and then turning it to \"*True*\" for subsequent runs. "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[1] Shiyu Liang, et al. *Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks*, ICLR 2018\n",
    "\n",
    "[2] Yarin Gal, et al. *Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning*, ICML 2015"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "9dfe7210",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As mentioned at the beginning of this notebook, to use the dataset from our paper (a subset of ImageNet-21k), first download ImageNet-21k in its entirety and use the *get_paper_dataset_info(path_to_ImageNet21k)* to trasnform it into our filtered version of the data.\n",
    "Define the ID dataset to be ImageNet-1k entire validation set.\n",
    "Finally, use the same code as we've seen above to evaluate with these datasets.\n",
    "For example:\n",
    "\n",
    "```\n",
    "from cood_uncertainty_lib import benchmark_model_on_cood_with_severities, get_paper_ood_dataset_info, \\\n",
    "    get_paper_id_dataset_info\n",
    "\n",
    "path_to_imagenet1k = 'D:\\ImageNet_1K\\ILSVRC2012_img_val'  # path to where you've saved ILSVRC2012_img_val\n",
    "paper_id_dataset = get_paper_id_dataset_info(path_to_imagenet1k, skip_scan=False)\n",
    "\n",
    "path_to_imagenet21k = 'D:\\fall11_whole'  # path to where you've saved fall11_whole, AKA, ImageNet-21k\n",
    "paper_ood_dataset = get_paper_ood_dataset_info(path_to_imagenet21k, skip_scan=True)\n",
    "\n",
    "\n",
    "results = benchmark_model_on_cood_with_severities(model='resnet18',\n",
    "                                                    confidence_function='softmax',\n",
    "                                                    cood_dataset_info=paper_id_dataset,\n",
    "                                                    id_dataset_info=paper_ood_dataset)\n",
    "```\n",
    "\n",
    "* When loading the paper's dataset with *get_paper_ood_dataset_info* \\ *get_paper_id_dataset_info*, the method scans the dataset to validate its integrity by default. This might take some time, *get_paper_ood_dataset_info* especially. The argument *skip_scan* controls whether or not to make this scan. We recommend leaving it \"*False*\" the first time loading these datasets, and then turning it to \"*True*\" for subsequent runs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1052a061",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c2e539",
   "metadata": {},
   "source": [
    "[1] Shiyu Liang, et al. *Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks*, ICLR 2018\n",
    "\n",
    "[2] Yarin Gal, et al. *Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning*, ICML 2015"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}